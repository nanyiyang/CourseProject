{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/nanyiyang/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver \n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import re \n",
    "import urllib\n",
    "import time\n",
    "from bs4.element import Comment\n",
    "import urllib.request\n",
    "\n",
    "import nltk\n",
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a webdriver object and set options for headless browsing\n",
    "options = Options()\n",
    "options.headless = True\n",
    "driver = webdriver.Chrome('./chromedriver',options=options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# citation: https://stackoverflow.com/questions/64109483/how-to-recognize-if-string-is-human-name/64109513\n",
    "def find_names(text):\n",
    "    \"\"\"returns a dictionary with keys as words identified as names\n",
    "\n",
    "    Keyword arguments:\n",
    "    text -- text string to find names in \n",
    "    \"\"\"\n",
    "    st = StanfordNERTagger('stanford-ner/classifiers/english.all.3class.distsim.crf.ser.gz', 'stanford-ner/stanford-ner.jar')\n",
    "    all_tags = {}\n",
    "\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        tokens = nltk.tokenize.word_tokenize(sent)\n",
    "        tags = st.tag(tokens)\n",
    "        for tag in tags:\n",
    "            if tag[1]=='PERSON':\n",
    "                all_tags[tag[0]] = 0\n",
    "                \n",
    "    return all_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(url, base_url, all_names):\n",
    "    \"\"\"Gets faculty URLs\n",
    "\n",
    "    Keyword arguments:\n",
    "    url -- the url of the faculty base directory\n",
    "    base_url -- the base url (used for relative links)\n",
    "    all_names -- the output of find_names\n",
    "    \"\"\"\n",
    "    driver.get(url)\n",
    "    res_html = driver.execute_script('return document.body.innerHTML')\n",
    "    soup = BeautifulSoup(res_html,'html.parser')\n",
    "\n",
    "    def is_in(text, name_dict):\n",
    "        if text is None:\n",
    "            return False\n",
    "        text_arr = text.split()\n",
    "        for word in text_arr:\n",
    "            if word in name_dict:\n",
    "                return True\n",
    "        return False\n",
    "        \n",
    "    names = []\n",
    "    faculty_links = []\n",
    "\n",
    "    for elem in soup.find_all(href=True, text = lambda text : is_in(text, all_names) ):\n",
    "        raw_link = elem['href']\n",
    "\n",
    "        # check if relative link or full link and adjust accordingly\n",
    "        if raw_link[0] == '/':\n",
    "            # relative link\n",
    "            formatted_link = base_url + raw_link\n",
    "        else:\n",
    "            formatted_link = raw_link\n",
    "        \n",
    "        faculty_links.append(formatted_link) \n",
    "        names.append(elem.getText())\n",
    "\n",
    "    print ('Found ',len(faculty_links),' faculty profiles!')\n",
    "    return faculty_links, names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# citation: https://stackoverflow.com/questions/1936466/beautifulsoup-grab-visible-webpage-text\n",
    "def tag_visible(element):\n",
    "    \"\"\"Checks if tag is visble\n",
    "    returns True or False\n",
    "\n",
    "    Keyword arguments:\n",
    "    element -- the element to check\n",
    "    \"\"\"\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def text_from_html(body):\n",
    "    \"\"\"Grabs all available text and returns it as a string\n",
    "\n",
    "    Keyword arguments:\n",
    "    body -- the body of the page\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(body, 'html.parser')\n",
    "    texts = soup.findAll(text=True)\n",
    "    visible_texts = filter(tag_visible, texts)\n",
    "    return u\" \".join(t.strip() for t in visible_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scrape_text(url):\n",
    "    \"\"\"Scrapes the text of the bio\n",
    "\n",
    "    Keyword arguments:\n",
    "    url -- url of the bio page\n",
    "    \"\"\"\n",
    "\n",
    "    # time.sleep(2)\n",
    "    bio_html = urllib.request.urlopen(url).read()\n",
    "    # print(text_from_html(html))\n",
    "    bio = text_from_html(bio_html)\n",
    "    bio = \" \".join(bio.split())\n",
    "    return bio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_from_url(url, base_url):\n",
    "    \"\"\"Main driver function.\n",
    "    Runs the scraping.\n",
    "\n",
    "    Keyword arguments:\n",
    "    url -- url of the faculty page\n",
    "    base_url -- base url of the faculty page (used for relative links)\n",
    "    \"\"\"\n",
    "    #create a webdriver\n",
    "    options = Options()\n",
    "    options.headless = True\n",
    "    driver = webdriver.Chrome('./chromedriver',options=options)\n",
    "\n",
    "    html = urllib.request.urlopen(url).read()\n",
    "\n",
    "    page_text = text_from_html(html)\n",
    "    page_text = \" \".join(page_text.split())\n",
    "    all_names = find_names(page_text)\n",
    "    \n",
    "    faculty_links, names = get_links(url, base_url, all_names)\n",
    "\n",
    "    bio_urls = []\n",
    "    bios = []\n",
    "\n",
    "    for i in range(len(faculty_links)):\n",
    "        print ('Scraping url {}/{}: {}'.format(i+1, len(faculty_links), names[i]))\n",
    "        faculty_url = faculty_links[i]\n",
    "        bio = scrape_text(faculty_url)\n",
    "\n",
    "        bio_urls.append(faculty_url.strip())\n",
    "        bios.append(bio)\n",
    "            \n",
    "    driver.close()\n",
    "    return names, bio_urls, bios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_data(names, bio_urls, bios):\n",
    "    \"\"\"Writes function to text file when given the output of scrape_from_url()\n",
    "\n",
    "    Keyword arguments:\n",
    "    names -- list of names\n",
    "    bio_urls -- list of urls of bios\n",
    "    bios -- list of bio text\n",
    "    \"\"\"\n",
    "    assert(len(names) == len(bio_urls))\n",
    "    assert(len(bios) == len(bio_urls))\n",
    "    \n",
    "    output = []\n",
    "    for i in range(len(names)):\n",
    "        output.append(names[i])\n",
    "        output.append(bio_urls[i])\n",
    "        output.append(bios[i])\n",
    "    print(len(output))\n",
    "    with open('output.txt','w') as f:\n",
    "        for l in output:\n",
    "            f.write(l)\n",
    "            f.write('\\n')\n",
    "    print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found  15  faculty profiles!\n",
      "Scraping url 1/15: Phil Colbert\n",
      "Scraping url 2/15: Brittany Erickson\n",
      "Scraping url 3/15: Arthur Farley\n",
      "Scraping url 4/15: Stephen Fickas\n",
      "Scraping url 5/15: Kathleen Freeman\n",
      "Scraping url 6/15: Michael Hennessy\n",
      "Scraping url 7/15: Anthony Hornof\n",
      "Scraping url 8/15: Jun Li\n",
      "Scraping url 9/15: Yingjiu Li\n",
      "Scraping url 10/15: Daniel Lowd\n",
      "Scraping url 11/15: Eugene Luks\n",
      "Scraping url 12/15: Allen Malony\n",
      "Scraping url 13/15: Andrzej Proskurowski\n",
      "Scraping url 14/15: Dave Wilkins\n",
      "Scraping url 15/15: Christopher Wilson\n"
     ]
    }
   ],
   "source": [
    "# url = \"https://cs.illinois.edu/about/people/all-faculty\"\n",
    "# base_url = 'https://cs.illinois.edu'\n",
    "\n",
    "url = \"https://cs.uoregon.edu/people/faculty\"\n",
    "base_url = 'https://cs.uoregon.edu/'\n",
    "\n",
    "names, bio_urls, bios = scrape_from_url(url, base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "export_data(names, bio_urls, bios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "17d1f3dac3e2042557ea3df2a999f60050fd45858a6bf61b815a86c911e69ee6"
  },
  "kernelspec": {
   "display_name": "Python 3.5.6 64-bit ('py35': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
